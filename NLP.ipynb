{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej9oHtnEuCoo"
      },
      "outputs": [],
      "source": [
        "# CoNLL-U (un format standard pour annoter des corpus linguistiques)\n",
        "def read_conllu_file(filepath):\n",
        "    # contiendra toutes les phrases\n",
        "    sentences = []\n",
        "    # temporaire, utilisée pour construire une phrase à la fois\n",
        "    sentence = []\n",
        "    #Lit le fichier ligne par ligne et enlève les espaces et retours à la ligne.\n",
        "    with open(filepath, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            # on ignore les commentaires\n",
        "            if line.startswith(\"#\") or line == \"\":\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "            # Les lignes valides sont découpées en colonnes\n",
        "            else:\n",
        "                parts = line.split('\\t')\n",
        "                # recupération du mot et sont etiquete\n",
        "                if len(parts) == 10:\n",
        "                    word = parts[1]\n",
        "                    pos_tag = parts[3]\n",
        "                    sentence.append((word, pos_tag))\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_dictionaries_fr(sentences):\n",
        "    #  (tag, mot) : Combien de fois un mot a été vu avec un certain tag\n",
        "    emission_counts = defaultdict(int)\n",
        "    # (prev_tag, tag) :Combien de fois un tag suit un autre tag\n",
        "    transition_counts = defaultdict(int)\n",
        "    # (tag) : Combien de fois chaque tag a été vu au total\n",
        "    tag_counts = defaultdict(int)\n",
        "    # calculer les proba d'emission et de trasition\n",
        "    for sent in sentences:\n",
        "        prev_tag = '--s--'\n",
        "        tag_counts[prev_tag] += 1  # Ajouter --s-- dans le comptage des tags\n",
        "        for word, tag in sent:\n",
        "            transition_counts[(prev_tag, tag)] += 1\n",
        "            emission_counts[(tag, word.lower())] += 1\n",
        "            tag_counts[tag] += 1\n",
        "            prev_tag = tag\n",
        "\n",
        "    return emission_counts, transition_counts, tag_counts\n",
        "# sert a garder que les mots frequents\n",
        "def build_vocab(emission_counts, min_freq=2):\n",
        "    word_freq = defaultdict(int)\n",
        "    # calculer la frequence des mots\n",
        "    for (tag, word), count in emission_counts.items():\n",
        "        word_freq[word] += count\n",
        "\n",
        "    # On ne garde que les mots qui apparaissent au moins min_freq fois\n",
        "    frequent_words = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
        "\n",
        "    # On trie les mots retenus (optionnel) puis on leur assigne un indice unique\n",
        "    vocab = {word: i for i, word in enumerate(sorted(frequent_words))}\n",
        "    return vocab\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# calculer les probabilités de transition entre les tags\n",
        "# paramètre de lissage de Laplace (évite des zéros dans la matrice)\n",
        "def create_transition_matrix_fr(alpha, tag_counts, transition_counts):\n",
        "    tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(tags)\n",
        "    tag_to_index = {tag: i for i, tag in enumerate(tags)}\n",
        "\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    for prev_tag in tags:\n",
        "        for curr_tag in tags:\n",
        "            i, j = tag_to_index[prev_tag], tag_to_index[curr_tag]\n",
        "            count = transition_counts.get((prev_tag, curr_tag), 0)\n",
        "            count_prev = tag_counts[prev_tag]\n",
        "            A[i, j] = (count + alpha) / (count_prev + alpha * num_tags)\n",
        "\n",
        "    return A, tag_to_index\n",
        "\n",
        "# calcule la proba qu'un mot soit emis par un tag\n",
        "def create_emission_matrix_fr(alpha, tag_counts, emission_counts, vocab):\n",
        "    tags = sorted(tag_counts.keys())\n",
        "    # nb de lignes de la matrice\n",
        "    num_tags = len(tags)\n",
        "    # nb de colonnes de la matrice\n",
        "    num_words = len(vocab)\n",
        "\n",
        "    tag_to_index = {tag: i for i, tag in enumerate(tags)}\n",
        "    word_to_index = vocab\n",
        "\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "    # parcourir les paires et ceux qui ne sont pas dans le vocab on les ignore\n",
        "    for (tag, word), count in emission_counts.items():\n",
        "        if word not in word_to_index:\n",
        "            continue\n",
        "        #  calcule de proba avec le lissage\n",
        "        i = tag_to_index[tag]\n",
        "        j = word_to_index[word]\n",
        "        B[i, j] = (count + alpha) / (tag_counts[tag] + alpha * num_words)\n",
        "\n",
        "    # gestion des valeurs nuls\n",
        "    # pour les cases nuls on applique que le lissage pour calculer la prob\n",
        "    for tag in tags:\n",
        "        i = tag_to_index[tag]\n",
        "        for word in vocab:\n",
        "            j = word_to_index[word]\n",
        "            if B[i, j] == 0:\n",
        "                B[i, j] = alpha / (tag_counts[tag] + alpha * num_words)\n",
        "\n",
        "    return B, tag_to_index, word_to_index\n",
        "\n",
        "import math\n",
        "# ========== Viterbi Forward ==========\n",
        "def viterbi_forward_fr(A, B, test_sentence, tag_to_index, word_to_index):\n",
        "    num_tags = len(tag_to_index)       # Nombre total de tags\n",
        "    m = len(test_sentence)            # Longueur de la phrase à étiqueter\n",
        "\n",
        "    # Initialisation de la matrice des meilleures probabilités et des meilleurs chemins\n",
        "    best_probs = np.full((num_tags, m), -np.inf)   # Probabilité log de chaque tag à chaque position\n",
        "    best_paths = np.zeros((num_tags, m), dtype=int)  # Indice du meilleur tag précédent\n",
        "\n",
        "    # Récupération de l’indice du tag de début (--s--)\n",
        "    s_idx = tag_to_index['--s--']\n",
        "\n",
        "    # Traitement du premier mot\n",
        "    first_word = test_sentence[0].lower()\n",
        "    word_index = word_to_index.get(first_word, None)\n",
        "\n",
        "    # Initialisation de la première colonne avec les transitions depuis le tag de début\n",
        "    for i in range(num_tags):\n",
        "        if A[s_idx, i] > 0:  # S'assurer que la transition est possible\n",
        "            emit_prob = B[i, word_index] if word_index is not None else 1e-6  # Gestion des mots inconnus\n",
        "            best_probs[i, 0] = math.log(A[s_idx, i]) + math.log(emit_prob)  # Log probabilité initiale\n",
        "\n",
        "    # Remplissage de la matrice best_probs et best_paths pour chaque mot suivant\n",
        "    for t in range(1, m):\n",
        "        word = test_sentence[t].lower()\n",
        "        word_index = word_to_index.get(word, None)\n",
        "\n",
        "        for j in range(num_tags):  # Tag courant\n",
        "            max_prob = -np.inf\n",
        "            best_k = 0\n",
        "            for k in range(num_tags):  # Tag précédent\n",
        "                trans_prob = A[k, j]  # Probabilité de transition de k vers j\n",
        "                emit_prob = B[j, word_index] if word_index is not None else 1e-6  # Émission avec fallback\n",
        "                prob = best_probs[k, t-1] + math.log(trans_prob) + math.log(emit_prob)  # Probabilité cumulée\n",
        "\n",
        "                # Mise à jour si meilleure probabilité trouvée\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_k = k\n",
        "            best_probs[j, t] = max_prob  # Meilleure probabilité pour tag j à position t\n",
        "            best_paths[j, t] = best_k    # Meilleur tag précédent menant à ce tag\n",
        "\n",
        "    return best_probs, best_paths\n",
        "\n",
        "# ========== Viterbi Backward ==========\n",
        "def viterbi_backward_fr(best_probs, best_paths, tag_to_index, states):\n",
        "    m = best_probs.shape[1]  # Longueur de la phrase\n",
        "\n",
        "    z = [None] * m       # Indices des tags prévus\n",
        "    pred = [None] * m    # Liste finale des prédictions de tags (sous forme de string)\n",
        "\n",
        "    # On commence par le tag ayant la plus grande probabilité à la dernière position\n",
        "    last_tag = np.argmax(best_probs[:, m-1])\n",
        "    z[m-1] = last_tag\n",
        "    pred[m-1] = states[last_tag]  # Conversion de l’indice en tag\n",
        "\n",
        "    # On remonte la chaîne en suivant les meilleurs chemins sauvegardés\n",
        "    for i in reversed(range(1, m)):\n",
        "        z[i-1] = best_paths[z[i], i]     # Récupération de l’indice du meilleur tag précédent\n",
        "        pred[i-1] = states[z[i-1]]       # Conversion en tag string\n",
        "\n",
        "    return pred  # Liste des tags prédits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDbdW93w6ujl"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def evaluate_model_accuracy(test_sentences, A, B, tag_to_index, word_to_index):\n",
        "    # Liste des états possibles (étiquettes), triés pour assurer un ordre cohérent\n",
        "    states = sorted(tag_to_index.keys())\n",
        "\n",
        "    # Initialisation des compteurs de bonnes prédictions et de total de mots\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Parcours de chaque phrase du jeu de test\n",
        "    for sentence in test_sentences:\n",
        "        # Séparation des mots et des étiquettes réelles\n",
        "        words = [word for word, true_tag in sentence]\n",
        "        true_tags = [true_tag for word, true_tag in sentence]\n",
        "\n",
        "        # Calcul des probabilités et des chemins via l'algorithme Viterbi (étape forward)\n",
        "        best_probs, best_paths = viterbi_forward_fr(A, B, words, tag_to_index, word_to_index)\n",
        "\n",
        "        # Récupération des étiquettes prédites via backtracking (étape backward)\n",
        "        pred_tags = viterbi_backward_fr(best_probs, best_paths, tag_to_index, states)\n",
        "\n",
        "        # Comparaison des étiquettes prédites avec les vraies pour chaque mot\n",
        "        for pred_tag, true_tag in zip(pred_tags, true_tags):\n",
        "            if pred_tag == true_tag:\n",
        "                correct += 1  # Bonne prédiction\n",
        "            total += 1      # Incrément du total\n",
        "\n",
        "    # Calcul du taux de précision (accuracy) sur tout le corpus\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def test_random_sentence(test_sentences, A, B, tag_to_index, word_to_index):\n",
        "    states = sorted(tag_to_index.keys())\n",
        "\n",
        "    # Choisir une phrase aléatoire du jeu de test\n",
        "    sentence = random.choice(test_sentences)\n",
        "    words = [word for word, tag in sentence]\n",
        "    true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "    print(\"=== Phrase testée ===\")\n",
        "    print(\" \".join(words))\n",
        "\n",
        "    # Appliquer Viterbi sur cette phrase pour obtenir les étiquettes prédites\n",
        "    best_probs, best_paths = viterbi_forward_fr(A, B, words, tag_to_index, word_to_index)\n",
        "    pred_tags = viterbi_backward_fr(best_probs, best_paths, tag_to_index, states)\n",
        "\n",
        "    # Affichage des résultats\n",
        "    print(\"\\nMots :\")\n",
        "    print(words)\n",
        "    print(\"\\nÉtiquettes réelles :\")\n",
        "    print(true_tags)\n",
        "    print(\"\\nÉtiquettes prédites :\")\n",
        "    print(pred_tags)\n",
        "\n",
        "    # Calcul et affichage de l'accuracy pour cette phrase uniquement\n",
        "    correct = sum(p == t for p, t in zip(pred_tags, true_tags))\n",
        "    total = len(true_tags)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"\\nAccuracy sur cette phrase : {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXPUowmtuLfv",
        "outputId": "d8b74370-1e72-475f-c522-ec9885fc8c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy du modèle sur tout le test set : 91.23%\n",
            "=== Phrase testée ===\n",
            "Comme en 2007 , le club local d' Arsenal remporte le trophée .\n",
            "\n",
            "Mots :\n",
            "['Comme', 'en', '2007', ',', 'le', 'club', 'local', \"d'\", 'Arsenal', 'remporte', 'le', 'trophée', '.']\n",
            "\n",
            "Étiquettes réelles :\n",
            "['COSUB', 'PREP', 'CHIF', 'PUNCT', 'DETMS', 'NMS', 'ADJMS', 'PREP', 'PROPN', 'VERB', 'DETMS', 'NMS', 'YPFOR']\n",
            "\n",
            "Étiquettes prédites :\n",
            "['COSUB', 'PREP', 'CHIF', 'PUNCT', 'DETMS', 'NMS', 'ADJMS', 'PREP', 'PROPN', 'VERB', 'DETMS', 'NMS', 'YPFOR']\n",
            "\n",
            "Accuracy sur cette phrase : 100.00%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Charger les données d'entraînement\n",
        "train_sentences = read_conllu_file(\"/content/train.conllu\")\n",
        "\n",
        "# 2. Créer les dictionnaires à partir de l'entraînement\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries_fr(train_sentences)\n",
        "\n",
        "# 3. Construire le vocabulaire\n",
        "vocab = build_vocab(emission_counts)\n",
        "\n",
        "# 4. Créer les matrices A (transition) et B (émission)\n",
        "alpha = 0.001  # paramètre de lissage\n",
        "A, tag_to_index = create_transition_matrix_fr(alpha, tag_counts, transition_counts)\n",
        "B, tag_to_index, word_to_index = create_emission_matrix_fr(alpha, tag_counts, emission_counts, vocab)\n",
        "\n",
        "# 5. Charger les données de test\n",
        "test_sentences = read_conllu_file(\"/content/test.conllu\")\n",
        "\n",
        "# 6. Évaluer le modèle\n",
        "acc = evaluate_model_accuracy(test_sentences, A, B, tag_to_index, word_to_index)\n",
        "print(f\"Accuracy du modèle sur tout le test set : {acc:.2%}\")\n",
        "\n",
        "# 7. Tester une phrase aléatoire\n",
        "test_random_sentence(test_sentences, A, B, tag_to_index, word_to_index)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}