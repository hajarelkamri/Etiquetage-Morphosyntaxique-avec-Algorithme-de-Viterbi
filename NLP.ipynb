{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6jC7RGSvHUuG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# ========== Lecture du fichier conllu ==========\n",
        "def read_conllu_file(filepath):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(filepath, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"#\") or line == \"\":\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "            else:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) == 10:\n",
        "                    word = parts[1]\n",
        "                    pos_tag = parts[3]\n",
        "                    sentence.append((word, pos_tag))\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "# ========== Comptage des transitions et émissions ==========\n",
        "def create_dictionaries_fr(sentences):\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "\n",
        "    for sent in sentences:\n",
        "        prev_tag = '--s--'\n",
        "        tag_counts[prev_tag] += 1\n",
        "        for word, tag in sent:\n",
        "            transition_counts[(prev_tag, tag)] += 1\n",
        "            emission_counts[(tag, word.lower())] += 1\n",
        "            tag_counts[tag] += 1\n",
        "            prev_tag = tag\n",
        "\n",
        "    return emission_counts, transition_counts, tag_counts\n",
        "\n",
        "# ========== Construction du vocabulaire avec <UNK> pour mots rares ==========\n",
        "def build_vocab(emission_counts, min_freq=2):\n",
        "    word_freq = defaultdict(int)\n",
        "    for (tag, word), count in emission_counts.items():\n",
        "        word_freq[word] += count\n",
        "\n",
        "    frequent_words = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
        "    vocab = {word: i for i, word in enumerate(sorted(frequent_words))}\n",
        "    vocab['<UNK>'] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "# ========== Matrice de transition ==========\n",
        "def create_transition_matrix_fr(alpha, tag_counts, transition_counts):\n",
        "    tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(tags)\n",
        "    tag_to_index = {tag: i for i, tag in enumerate(tags)}\n",
        "\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    for prev_tag in tags:\n",
        "        for curr_tag in tags:\n",
        "            i, j = tag_to_index[prev_tag], tag_to_index[curr_tag]\n",
        "            count = transition_counts.get((prev_tag, curr_tag), 0)\n",
        "            count_prev = tag_counts[prev_tag]\n",
        "            A[i, j] = (count + alpha) / (count_prev + alpha * num_tags)\n",
        "\n",
        "    return A, tag_to_index\n",
        "\n",
        "# ========== Matrice d’émission avec <UNK> ==========\n",
        "def create_emission_matrix_fr(alpha, tag_counts, emission_counts, vocab):\n",
        "    tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(tags)\n",
        "    num_words = len(vocab)\n",
        "    tag_to_index = {tag: i for i, tag in enumerate(tags)}\n",
        "    word_to_index = vocab\n",
        "\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    for (tag, word), count in emission_counts.items():\n",
        "        word = word if word in word_to_index else '<UNK>'\n",
        "        i = tag_to_index[tag]\n",
        "        j = word_to_index[word]\n",
        "        B[i, j] += count\n",
        "\n",
        "    # Normalisation avec lissage\n",
        "    for tag in tags:\n",
        "        i = tag_to_index[tag]\n",
        "        total = sum(B[i, :]) + alpha * num_words\n",
        "        for word in vocab:\n",
        "            j = word_to_index[word]\n",
        "            B[i, j] = (B[i, j] + alpha) / total\n",
        "\n",
        "    return B, tag_to_index, word_to_index\n",
        "\n",
        "# ========== Viterbi Forward ==========\n",
        "def viterbi_forward_fr(A, B, test_sentence, tag_to_index, word_to_index):\n",
        "    num_tags = len(tag_to_index)\n",
        "    m = len(test_sentence)\n",
        "    best_probs = np.full((num_tags, m), -np.inf)\n",
        "    best_paths = np.zeros((num_tags, m), dtype=int)\n",
        "\n",
        "    s_idx = tag_to_index['--s--']\n",
        "\n",
        "    first_word = test_sentence[0].lower()\n",
        "    word_index = word_to_index.get(first_word, word_to_index['<UNK>'])\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        if A[s_idx, i] > 0:\n",
        "            emit_prob = B[i, word_index]\n",
        "            best_probs[i, 0] = math.log(A[s_idx, i]) + math.log(emit_prob)\n",
        "\n",
        "    for t in range(1, m):\n",
        "        word = test_sentence[t].lower()\n",
        "        word_index = word_to_index.get(word, word_to_index['<UNK>'])\n",
        "\n",
        "        for j in range(num_tags):\n",
        "            max_prob = -np.inf\n",
        "            best_k = 0\n",
        "            for k in range(num_tags):\n",
        "                trans_prob = A[k, j]\n",
        "                emit_prob = B[j, word_index]\n",
        "                prob = best_probs[k, t-1] + math.log(trans_prob) + math.log(emit_prob)\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_k = k\n",
        "            best_probs[j, t] = max_prob\n",
        "            best_paths[j, t] = best_k\n",
        "\n",
        "    return best_probs, best_paths\n",
        "\n",
        "# ========== Viterbi Backward ==========\n",
        "def viterbi_backward_fr(best_probs, best_paths, tag_to_index, states):\n",
        "    m = best_probs.shape[1]\n",
        "    z = [None] * m\n",
        "    pred = [None] * m\n",
        "\n",
        "    last_tag = np.argmax(best_probs[:, m-1])\n",
        "    z[m-1] = last_tag\n",
        "    pred[m-1] = states[last_tag]\n",
        "\n",
        "    for i in reversed(range(1, m)):\n",
        "        z[i-1] = best_paths[z[i], i]\n",
        "        pred[i-1] = states[z[i-1]]\n",
        "\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Vdv0E5BaPnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def evaluate_model_accuracy(test_sentences, A, B, tag_to_index, word_to_index):\n",
        "    states = sorted(tag_to_index.keys())\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        words = [word for word, true_tag in sentence]\n",
        "        true_tags = [true_tag for word, true_tag in sentence]\n",
        "\n",
        "        best_probs, best_paths = viterbi_forward_fr(A, B, words, tag_to_index, word_to_index)\n",
        "        pred_tags = viterbi_backward_fr(best_probs, best_paths, tag_to_index, states)\n",
        "\n",
        "        for pred_tag, true_tag in zip(pred_tags, true_tags):\n",
        "            if pred_tag == true_tag:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def test_random_sentence(test_sentences, A, B, tag_to_index, word_to_index):\n",
        "    states = sorted(tag_to_index.keys())\n",
        "    sentence = random.choice(test_sentences)\n",
        "    words = [word for word, tag in sentence]\n",
        "    true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "    print(\"=== Phrase testée ===\")\n",
        "    print(\" \".join(words))\n",
        "\n",
        "    best_probs, best_paths = viterbi_forward_fr(A, B, words, tag_to_index, word_to_index)\n",
        "    pred_tags = viterbi_backward_fr(best_probs, best_paths, tag_to_index, states)\n",
        "\n",
        "    print(\"\\nMots :\")\n",
        "    print(words)\n",
        "    print(\"\\nÉtiquettes réelles :\")\n",
        "    print(true_tags)\n",
        "    print(\"\\nÉtiquettes prédites :\")\n",
        "    print(pred_tags)\n",
        "\n",
        "    correct = sum(p == t for p, t in zip(pred_tags, true_tags))\n",
        "    total = len(true_tags)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"\\nAccuracy sur cette phrase : {accuracy:.2%}\")\n",
        "\n",
        "\n",
        "def test_custom_sentence(custom_sentence, A, B, tag_to_index, word_to_index):\n",
        "    states = sorted(tag_to_index.keys())\n",
        "    words = custom_sentence.split()\n",
        "\n",
        "    print(\"=== Phrase testée ===\")\n",
        "    print(\" \".join(words))\n",
        "\n",
        "    best_probs, best_paths = viterbi_forward_fr(A, B, words, tag_to_index, word_to_index)\n",
        "    pred_tags = viterbi_backward_fr(best_probs, best_paths, tag_to_index, states)\n",
        "\n",
        "    print(\"\\nMots :\")\n",
        "    print(words)\n",
        "    print(\"\\nÉtiquettes prédites :\")\n",
        "    print(pred_tags)\n"
      ],
      "metadata": {
        "id": "2tytOWY-TDdw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Chargement des données d'entraînement =========\n",
        "train_sentences = read_conllu_file(\"/content/train.conllu\")\n",
        "\n",
        "# Création des dictionnaires\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries_fr(train_sentences)\n",
        "\n",
        "# Création du vocabulaire\n",
        "vocab = build_vocab(emission_counts, min_freq=2)\n",
        "\n",
        "# Smoothing\n",
        "alpha = 0.001\n",
        "\n",
        "# Matrices de transition et d'émission\n",
        "A, tag_to_index = create_transition_matrix_fr(alpha, tag_counts, transition_counts)\n",
        "B, tag_to_index, word_to_index = create_emission_matrix_fr(alpha, tag_counts, emission_counts, vocab)\n",
        "\n",
        "# ========= Évaluation globale sur le test =========\n",
        "test_sentences = read_conllu_file(\"/content/test.conllu\")\n",
        "accuracy = evaluate_model_accuracy(test_sentences, A, B, tag_to_index, word_to_index)\n",
        "print(f\"\\nAccuracy du modèle sur l'ensemble du fichier test : {accuracy:.2%}\\n\")\n",
        "\n",
        "# ========= Test d’une phrase aléatoire =========\n",
        "dev_sentences = read_conllu_file(\"/content/dev.conllu\")\n",
        "test_random_sentence(dev_sentences, A, B, tag_to_index, word_to_index)\n"
      ],
      "metadata": {
        "id": "NBQb5MxASab5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkTOl8tYSaip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BidHdON-NHC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1ZeMs6Qe6zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kM1sL_LNe64s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVWQ5E5Ye67H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oo46xqfKe6-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLwMzwUYe7Dm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}